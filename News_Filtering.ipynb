{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Import Lib"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install wikipedia","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting wikipedia\n  Downloading wikipedia-1.4.0.tar.gz (27 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.6/site-packages (from wikipedia) (4.8.2)\nRequirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from wikipedia) (2.22.0)\nRequirement already satisfied: soupsieve>=1.2 in /opt/conda/lib/python3.6/site-packages (from beautifulsoup4->wikipedia) (1.9.5)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.25.8)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\nBuilding wheels for collected packages: wikipedia\n  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11685 sha256=769d50c946959a65f9d162443298edef35c27a2fcac84c0aeaed22f345059b81\n  Stored in directory: /root/.cache/pip/wheels/e4/26/c3/4f401d3e5c907d26a2c4ca8484141f02f7630c98fbcfd78a5e\nSuccessfully built wikipedia\nInstalling collected packages: wikipedia\nSuccessfully installed wikipedia-1.4.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport pandas as pd\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\nimport numpy as np\nimport time\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\nimport pickle\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport math\n\nimport sys,os,os.path\nimport wikipedia","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Loading News Data**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"raw_data = pd.read_json(\"/kaggle/input/signalmedia/sample-1M.jsonl\",\n                        lines=True,\n                        orient='columns')\nprint(raw_data.shape)\nraw_data.head()","execution_count":3,"outputs":[{"output_type":"stream","text":"(1000000, 6)\n","name":"stdout"},{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"                                     id  \\\n0  f7ca322d-c3e8-40d2-841f-9d7250ac72ca   \n1  609772bc-0672-4db5-8516-4c025cfd54ca   \n2  1aa9d1b0-e6ba-4a48-ad0c-66552d896aac   \n3  719699f9-47be-4bc7-969b-b53a881c95ae   \n4  a080f99a-07d9-47d1-8244-26a540017b7a   \n\n                                             content  \\\n0  VETERANS saluted Worcester's first ever breakf...   \n1  New Product Gives Marketers Access to Real Key...   \n2  Home »\\rStyle » The Return Of The Nike Air Max...   \n3  NYMag.com Daily Intelligencer Vulture The Cut ...   \n4  KUALA LUMPUR, Sept 15 (MySinchew) -- The Kuala...   \n\n                                               title media-type  \\\n0  Worcester breakfast club for veterans gives hu...       News   \n1  Jumpshot Gives Marketers Renewed Visibility In...       News   \n2  The Return Of The Nike Air Max Sensation Has 8...       Blog   \n3   This New Dating App Will Ruin Your Internet Game       Blog   \n4                  Pay up or face legal action: DBKL       News   \n\n                             source             published  \n0               Redditch Advertiser  2015-09-07T10:16:14Z  \n1  Virtualization Conference & Expo  2015-09-17T15:00:00Z  \n2                   Streets Connect  2015-09-22T22:54:37Z  \n3                           The Cut  2015-09-16T23:12:11Z  \n4                        My Sinchew  2015-09-15T10:17:53Z  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>content</th>\n      <th>title</th>\n      <th>media-type</th>\n      <th>source</th>\n      <th>published</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>f7ca322d-c3e8-40d2-841f-9d7250ac72ca</td>\n      <td>VETERANS saluted Worcester's first ever breakf...</td>\n      <td>Worcester breakfast club for veterans gives hu...</td>\n      <td>News</td>\n      <td>Redditch Advertiser</td>\n      <td>2015-09-07T10:16:14Z</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>609772bc-0672-4db5-8516-4c025cfd54ca</td>\n      <td>New Product Gives Marketers Access to Real Key...</td>\n      <td>Jumpshot Gives Marketers Renewed Visibility In...</td>\n      <td>News</td>\n      <td>Virtualization Conference &amp; Expo</td>\n      <td>2015-09-17T15:00:00Z</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1aa9d1b0-e6ba-4a48-ad0c-66552d896aac</td>\n      <td>Home »\\rStyle » The Return Of The Nike Air Max...</td>\n      <td>The Return Of The Nike Air Max Sensation Has 8...</td>\n      <td>Blog</td>\n      <td>Streets Connect</td>\n      <td>2015-09-22T22:54:37Z</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>719699f9-47be-4bc7-969b-b53a881c95ae</td>\n      <td>NYMag.com Daily Intelligencer Vulture The Cut ...</td>\n      <td>This New Dating App Will Ruin Your Internet Game</td>\n      <td>Blog</td>\n      <td>The Cut</td>\n      <td>2015-09-16T23:12:11Z</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>a080f99a-07d9-47d1-8244-26a540017b7a</td>\n      <td>KUALA LUMPUR, Sept 15 (MySinchew) -- The Kuala...</td>\n      <td>Pay up or face legal action: DBKL</td>\n      <td>News</td>\n      <td>My Sinchew</td>\n      <td>2015-09-15T10:17:53Z</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":" ## Text Preprocessing "},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"def Text_Cleaning(data, remove_numbers = True, custom_stopwords = None , remove_stop_words=True, lemmatize_words = False, stem_words= False):\n\n    \"\"\"\n\n    Text Cleaning  \n    Input: List of Documents    Output: Cleaned List of Documents\n    Parameters: \n    custom_stopwords - Custom List of Stopwords(should be passed as a list)\n    remove_stop_words - Binary parameter (True/False) to remove stopwords or do not want to perform this. Default is True\n    lemmatize_words - Binary parameter (True/False) for Lemmatization (WordNet Lemmatizer used) or not. Default is False\n    stem_words - Binary parameter (True/False) for Stemming (Lancaster Stemmer used) or not. Default is False\n\n    \"\"\"\n    import os\n\n    package = \"nltk\"\n\n    try:\n        __import__package\n    except:\n        os.system(\"pip install \"+ package)\n        import nltk\n        nltk.download('stopwords')\n        nltk.download('wordnet')\n    \n    import nltk\n    import pandas as pd\n    import string\n    \n    \n\n    import re\n    data = [re.sub(\"^\\s+\",\"\",x) for x in data] # removes leading white spaces\n    data = [re.sub(\"\\s+\\Z\",\"\",x) for x in data] # removes trailing white spaces\n\n    data = [x.lower() for x in data]\n    data = [x.translate(str.maketrans(' ',' ', '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^`{|}~_')) for x in data]\n    data = [x.replace('-', ' ') for x in data]\n    \n    if remove_numbers:\n        data = [x.translate(str.maketrans('','', string.digits)) for x in data]\n\n\n    ### Tokenizer\n    def tokenize_word_text(text):\n        \"\"\"\n        Sentence tokenize in NLTK with sent_tokenize \n        The sent_tokenize function uses an instance of NLTK known as PunktSentenceTokenizer\n        This instance of NLTK has already been trained to perform tokenization on different European languages on the basis \n        of letters or punctuation that mark the beginning and end of sentences\n        \"\"\"\n        from nltk.tokenize import WordPunctTokenizer \n        tokenizer = WordPunctTokenizer()\n        return tokenizer.tokenize(text)\n\n\n    ## Removing Stopwords\n    if remove_stop_words:\n        roman_numerals = ['iii', 'iiv', 'iiiv', 'vii', 'viii', 'xii', 'xiii', 'xxi','xxii', 'xxiii', 'xxv', 'xxvi', 'xxvii'] #exclude 2char numerals\n        temporal_words = ['hour', 'day', 'week', 'month', 'quarter', 'year', 'annual', 'today', 'tomorrow', 'yesterday', \n                          'prior', 'prio', 'preceed', 'yet', \"before\", 'after', 'earlier', 'later', 'ago', 'this', 'next', 'last',\n                          'once-daili', 'once-month', 'once-through', 'once-week', 'once', 'also',\n                          'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday',\n                          'jan', 'feb', 'apr', 'jun', 'jul', 'aug', 'sept', 'oct', 'nov', 'dec', 'may', 'onmay', 'postmay'\n                         ]\n        from nltk.corpus import stopwords\n        stopword_list = stopwords.words('english')\n        if custom_stopwords:\n            stop_list = set(stopword_list + custom_stopwords + roman_numerals + temporal_words)\n        else:\n            stop_list = set(stopword_list + roman_numerals + temporal_words)\n\n        def removeStopwords_before_tokens(text, stopwords):\n            tokens = tokenize_word_text(text)\n            return ' '.join([w for w in tokens if w not in stopwords])\n\n        data = [removeStopwords_before_tokens(x, stop_list) for x in data]\n\n\n\n    ##### Word Lemmatization Using WordNet\n    def wordlemmatize(text):\n        from nltk.stem.wordnet import WordNetLemmatizer \n        lem = WordNetLemmatizer()\n        tokens = tokenize_word_text(text)\n        return ' '.join([lem.lemmatize(w) for w in tokens])\n    if lemmatize_words:\n        data = [wordlemmatize(x) for x in data]\n\n\n\n    ##### Word Stemming Using Lancaster Stemmer\n    def wordstem(text):\n        from nltk.stem.porter import PorterStemmer\n        ls = PorterStemmer()\n        tokens = tokenize_word_text(text)\n        return ' '.join([ls.stem(w) for w in tokens])\n    if stem_words:\n        data = [wordstem(x) for x in data]\n\n\n   \n    return data","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\nraw_data['clean_title'] = Text_Cleaning(raw_data['title'],lemmatize_words = True)","execution_count":5,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\nCPU times: user 1min 58s, sys: 401 ms, total: 1min 59s\nWall time: 2min 6s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# News Filtering for topic Emission\n\nPropsed Startegy : The objective of this part is to filter the news for a any topic. In this case we have only one topic 'Emission'. In order to get the topic releavnt news, I have built an independinet pipeline which can work for any topic.\n\n** Working\n\n-Create a topic relevant corpus by extracting relevant wikipedia pages.In order to understand what pages on wikipedia needs to be searched, use any pre-trained embedding model and get most similar words(i.e top 5) . The most similar words can be used for selection on wiki pages.\n\nOnce the corpus is built,use TF-IDF vectorizer to extract important words by filtering on TF-IDF scores . \nNow using any pre-trained embedding model calcualte the cosine similarity between the TF-IDF output words embeddings and Topic's most similar(top5 ) words. Words with coaines simialrity of more tha  60% are our shortlisted keyword list\n"},{"metadata":{},"cell_type":"markdown","source":"## Topic relevant corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"def Corpus_Generation(words,lemmatize=False,pos_tagging = ['NOUN','ADJ']):\n    \n    \"\"\"\n\n    \n    Input: List of one or more words   Output: List of Document corresponding to each word\n    \n    Parameters: \n    lemmatize_words - Binary parameter (True/False) for Lemmatization (WordNet Lemmatizer used) or not. Default is False   \n    pos_tagging - List of Part-of-speech tags(spaCy model used : 'en_core_web_sm') by using English POS tag set from https://spacy.io/api/annotation. Default is None\n    \n    \n\n    \"\"\"  \n   \n    if isinstance(words,str):\n        theme_words  = []\n        theme_words.append(words)\n    else : theme_words = words  \n        \n\n    \n    # Wikipedia page extraction\n    theme_text = []\n    for data1 in theme_words:\n        try :\n            theme_words = []        \n            data = wikipedia.page(data1)\n            print('Searching Wikipedia page for theme word : ',data1)\n            print('Wikipedia page found : ',data.title)\n\n            for token in nlp(data.content):\n                t=token.text\n                if lemmatize :           # Word Lemmatization\n                    t = token.lemma_\n                if pos_tagging:            # Part of Speech tagging\n                    if token.pos_ in pos_tagging:                    \n                        theme_words.append(t)\n                else : theme_words.append(t)\n            theme_text.append(\" \".join(theme_words))       \n            print('----------Corpus created---------------')\n            print(str(\" \".join(theme_words))[0:1000])\n        except wikipedia.exceptions.PageError : print('!! ',data1,' not found on wikipedia.')\n        except wikipedia.exceptions.DisambiguationError : print('!! ',data1,' not found on wikipedia.')\n        except JSONDecodeError : print('!!  error on http server ')    \n            \n                                 \n    return(theme_text)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm')\nimport nltk\nimport string\nimport numpy as np    \nfrom sklearn.feature_extraction.text import TfidfVectorizer        \n ","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extract wiki relevant pages using pre-trained google news model\npre-trained model location : https://code.google.com/archive/p/word2vec/"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":10,"outputs":[{"output_type":"stream","text":"/kaggle/input/signalmedia/sample-1M.jsonl\n/kaggle/input/signalmedia/GoogleNews-vectors-negative300.bin\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gensim\ngoogle_pretrained_model = gensim.models.KeyedVectors.load_word2vec_format('/kaggle/input/signalmedia/GoogleNews-vectors-negative300.bin',binary=True)   \ngoogle_pretrained_model.most_similar('emission')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Output : \n    \n[('emissions', 0.8727598190307617),\n ('carbon_emission', 0.7887347340583801),\n ('greenhouse_gas_emission', 0.7638275623321533),\n ('CO2_emission', 0.7512905597686768),\n ('emmissions', 0.7460930943489075),\n ('emission_reduction', 0.7448598146438599),\n ('carbon_dioxide_emission', 0.7386904954910278),\n ('Emission', 0.7283594608306885),\n ('greenhouse_gas_emissions', 0.7274933457374573),\n ('greenhouse_gas', 0.7139266729354858)]"},{"metadata":{"trusted":true},"cell_type":"code","source":"wiki_pages = ['Carbon footprint','Greenhouse gas','Emission_intensity','Vehicle_emissions_control','Emissions_trading','Carbon_emissions_reporting']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"Theme_Corpus = Corpus_Generation(wiki_pages,lemmatize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## TF-IDF filtering for imporatant words : \nThis is done by calcualting tf-idf scores of our wiki generated corpus against 100 random wikipages. "},{"metadata":{"trusted":true},"cell_type":"code","source":"if isinstance(Theme_Corpus,list):\n    Topic_Corpus = \" \".join(Theme_Corpus)\n\n### tf_idf algorithm ------------------------------ starts :\n\n#Function definition for TF-IDF Algorithm\ndef tf_idf(data,ngrams,lowercase,min_word_len):\n    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,ngrams),token_pattern=r'\\b\\w+\\b',smooth_idf=False, lowercase =lowercase)\n    tfidf=tfidf_vectorizer.fit_transform(data).toarray()\n    final_tfidf=np.transpose(tfidf)\n    final_tfidf= np.around( final_tfidf,decimals=3)\n    final_tfidf=pd.DataFrame(final_tfidf,index=tfidf_vectorizer.get_feature_names())\n    final_tfidf = pd.DataFrame(final_tfidf[len(final_tfidf.columns)-1]).reset_index()\n    final_tfidf.columns = ['words', 'tfidf_score']\n    final_tfidf['len'] = [len(x) for x in final_tfidf['words']]\n    final_tfidf = final_tfidf.loc[final_tfidf.len>=min_word_len,['words','tfidf_score']]\n    final_tfidf = final_tfidf.sort_values('tfidf_score',ascending=False)        \n    return final_tfidf\n\nprint('---------------Performing TF-IDF calculations---------------')\n\n\n\n# Perform TF-IDF with random 100 wikipedia pages \nrandom_page_names = [wikipedia.random(1) for i in range(100)]\nrandom_page_names =set(random_page_names)\nrandom_pages_content = []\nfor pages in random_page_names:\n    #print(pages)\n    theme_words = []\n    try:\n        data = wikipedia.page(pages)\n        for token in nlp(data.content):\n            t=token.text\n            t = token.lemma_\n            theme_words.append(t)\n    except wikipedia.exceptions.DisambiguationError: theme_words = []\n    except wikipedia.exceptions.PageError: theme_words = []  \n    #except JSONDecodeError : \n\n    random_pages_content.append(\" \".join(theme_words))      \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"random_pages_content.append(Topic_Corpus)\nfinal_tfidf_rnd = tf_idf(random_pages_content,ngrams=2,min_word_len=3,lowercase=True)\n\n# Filtering words with tf-idf score >0 \nfiltered_unique_words = final_tfidf_rnd[final_tfidf_rnd.tfidf_score >0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Perfroming cosine similarity on word embeddings to get context similar words\nNot all words are important to get relevant news, hence we will need only sematically similar words for filtering news"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading pre-trained vocabulary and cleaning the words to find out if our tf-idf out words are available\nmodel_vocab = pd.DataFrame(list(google_pretrained_model.vocab))\n#cleaning words\nmodel_vocab['clean_word'] = [x.translate(str.maketrans('','', '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^`{|}~')).strip().lower() for x in model_vocab[0]]\nmodel_vocab['model'] = 'google_news_ngrams_trained'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cleaning tf-df words in order to match against model vocabulary\ntf_idf_words = list(set(filtered_unique_words.words))\nk2 = pd.DataFrame(tf_idf_words)\nk2['clean1'] = [str(seed).translate(str.maketrans('','', '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^`{|}~')).strip().replace(' ','_').lower() for seed in tf_idf_words]\nk2['clean2'] = [str(seed).translate(str.maketrans('','', '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^`{|}~')).strip().replace(' ','').replace('_','').lower() for seed in tf_idf_words]\nk2.columns = ['orig1','clean1','clean2']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Searching tf-idf word list in model vocab\nw2v1 = model_vocab[model_vocab.clean_word.isin(k2.clean1)].reset_index(drop=True)\nw2v1 = w2v1.drop_duplicates(['clean_word'], keep='first')\nw2v11 = pd.merge(k2,w2v1,left_on='clean1',right_on='clean_word',how='left')\n\ntemp = w2v11.loc[w2v11.model.isna()==True,['orig1','clean1','clean2']].reset_index(drop=True)\nw2v2  = model_vocab[model_vocab.clean_word.isin(temp.clean2)].reset_index(drop=True)\nw2v2 = w2v2.drop_duplicates(['clean_word'], keep='first')\nw2v22 = pd.merge(temp,w2v2,left_on='clean2',right_on='clean_word',how='left')\n\nwords_embed = w2v11[~(w2v11['model'].isna())].append(w2v22[~(w2v22['model'].isna())])\nwords_embed = words_embed.rename(columns={0:'model_orig_word'})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# List of only those words whose embeddong is available\nx = words_embed.drop_duplicates('model_orig_word')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function for similarity matrix\ndef similarity_with_seed_words(words,seedwords,model):\n    print('============Calculating Similarity scores===========')\n    start_time = time.time()\n    similarity_matrix=pd.DataFrame(index=words, columns=seedwords)\n    for index,row in similarity_matrix.iterrows(): \n        #print(index)\n        for column in similarity_matrix.columns:\n            #print(column)\n            try:\n                similarity_matrix.loc[index,column]=model.similarity(column, index)  \n\n            except KeyError:\n                similarity_matrix.loc[index,column] = 0  \n    print(\"--- %s seconds ---\" , time.time() - start_time)                  \n    return(similarity_matrix)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculating similarity of embeddings\nthreshold_w2v = 0.5\nwords_w2v = []\nseed_words = ['emissions','carbon_emission','greenhouse_gas_emission']\n\nsim1 = similarity_with_seed_words(x.loc[x.model=='google_news_ngrams_trained','model_orig_word'],seed_words,google_pretrained_model)\nfor column in sim1.columns :\n    words_w2v.extend(sim1[sim1[column] >= threshold_w2v].index)\n\nkeywords = set(words_w2v) \nprint(len(keywords))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"print(keywords) : \n{'CO2',\n 'CO2_emission',\n 'Carbon_Offset',\n 'Clean_Air',\n 'GHG',\n 'GHG_emission',\n 'NOx',\n 'NOx_emission',\n 'air_pollution',\n 'carbon',\n 'carbon_dioxide',\n 'carbon_emission',\n 'carbon_emissions',\n 'carbon_footprint',\n 'carbon_neutral',\n 'carbon_offsetting',\n 'climate_change',\n 'emission',\n 'emission_reduction',\n 'emissions',\n 'exhaust_emission',\n 'fossil_fuel',\n 'global_warming',\n 'greenhouse_gas',\n 'nitrogen_oxide',\n 'particulate_emission',\n 'particulate_matter',\n 'pollutant',\n 'polluter',\n 'pollution',\n 'renewable_energy',\n 'smog',\n 'soot',\n 'sulfur_dioxide',\n 'tailpipe_emission',\n 'tailpipe_exhaust'}"},{"metadata":{},"cell_type":"markdown","source":"## Filter News title with relevant keywords"},{"metadata":{"trusted":true},"cell_type":"code","source":"def incidence_matrix(docs,keywords,incidence_matrix=True):\n    vectorizer1 = TfidfVectorizer(vocabulary=keywords,use_idf=False,ngram_range=(1,2),binary=incidence_matrix,lowercase=True,norm=False)\n    tfidf = vectorizer1.fit_transform(docs).toarray()\n    incidence_matrix = pd.DataFrame(tfidf,columns  = vectorizer1.get_feature_names())\n    total_word_hit = incidence_matrix.sum(axis = 1)\n    print('Word hit stats :')\n    print(total_word_hit.value_counts())\n    return(total_word_hit,incidence_matrix)\n\n# Keyword hit\nbd_hits,y1 = incidence_matrix(raw_data['clean_title'],keywords,incidence_matrix=True)\nraw_data['keyword_hit'] = bd_hits\nraw_data['keyword_hit'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Topic releavnt News filtering\nfilter_df = raw_data[raw_data.keyword_hit>=1].reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# NER Tagging to extract the main company names "},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# NER tagging in Content\ndf1 = pd.DataFrame()\n\nfor i in range(filter_df.shape[0]-1):  \n    print(i)\n    for ent in nlp(filter_df.iloc[i,1]).ents:\n        if len(ent.text) >0:\n            x = pd.DataFrame([ent.text,ent.label_]).T\n            x['id'] = filter_df.iloc[i,0]\n            df1 = df1.append(x)\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_content_orgs = df1[df1[1]=='ORG'].reset_index(drop=True)\ndf_content_orgs.columns = ['Company_Content','label','id']\ndf_content_orgs.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# NER tagging in Title\ndf_title = pd.DataFrame()\n\nfor i in range(filter_df.shape[0]-1):  \n    print(i)\n    for ent in nlp(filter_df.iloc[i,2]).ents:\n        if len(ent.text) >0:\n            x = pd.DataFrame([ent.text,ent.label_]).T\n            x['id'] = filter_df.iloc[i,0]\n            df_title = df_title.append(x)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_title.columns = ['Company_Title','label','id']\ndf_title_orgs =  df_title[df_title['label']=='ORG'].reset_index(drop=True)\ndf_title_orgs.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Assumption : Main company are only those tags which are simialr in content and title "},{"metadata":{"trusted":true},"cell_type":"code","source":"companies_group = df_title_orgs.merge(df_content_orgs,on=['id'],how='inner')\ncompanies_group['match']= companies_group['Company_Content'].str.lower()==companies_group['Company_Title'].str.lower()\ncompanies_group = companies_group[companies_group['match']==True]\ncompanies_group['Company_Name'] = companies_group['Company_Title'].str.lower()\nunique_comps = companies_group.drop_duplicates(['id','Company_Name']).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"unique_companies_list = unique_comps['Company_Name'].unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Company specific Text summarization using TextRank"},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install summa\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from summa import summarizer\nnews_combined = []\nnews_summary = []\nfor i in unique_companies_list :\n    print(i)\n    text = ' . '.join(filter_df.loc[filter_df['id'].isin(unique_comps.loc[unique_comps.Company_Name==i]['id'].unique()),'content'])\n    news_combined.append(text)\n    news_summary.append(summarizer.summarize(text))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc = nlp(news_summary[1])\ndisplacy.serve(doc, style=\"ent\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}