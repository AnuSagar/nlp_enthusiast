{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ndf_tweet = pd.DataFrame()\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        filename = os.path.join(dirname,filename)\n        if 'Tweets' in filename:\n            df_tweet = df_tweet.append(pd.read_csv(filename))\n            print('added : ',filename)\n        \n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\ndf_hashtags = pd.read_csv('/kaggle/input/coronavirus-covid19-tweets/Hashtags.CSV')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hash_tag_useful = df_hashtags.hashtag.value_counts().reset_index()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" hash_tag_useful= hash_tag_useful[hash_tag_useful.hashtag>1000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"hash_tag_useful.to_csv('hastags.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pwd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_country = pd.read_csv('/kaggle/input/coronavirus-covid19-tweets/Countries.CSV')\ndf_country.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_country[df_country.country_code.isin(['US','GB','IN','ES','CA'])]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Top 5 country tweets\")\nctry = df_tweet.country_code.value_counts()\nctry.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Only on top 5\ndf_tweet=df_tweet[df_tweet.country_code.isin(['US','CA','IN','GB','ES'])].reset_index(drop=True)\ndf_tweet.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet.verified.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet.place_full_name.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install vaderSentiment","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\nanalyser = SentimentIntensityAnalyzer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sentiment_analyzer_Score(sentence):\n    score = analyser.polarity_scores(sentence)\n    return(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_tweet['neg_score'] = 0\n# df_tweet['pos_score'] = 0\n# df_tweet['neu_score'] = 0\ndf_tweet['composite_score'] = 0\nfor i in range(0,len(df_tweet)-1):\n    print(i)\n    s = sentiment_analyzer_Score(df_tweet.iloc[i,4])\n#     df_tweet.iloc[i,22] = s['neg']\n#     df_tweet.iloc[i,23] = s['neu']\n#     df_tweet.iloc[i,24] = s['pos']\n    df_tweet.iloc[i,22] = s['compound']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet.loc[df_tweet.composite_score>=0.05,'sentiment'] = 'positive'\ndf_tweet.loc[df_tweet.composite_score<-0.05,'sentiment'] = 'negative'\ndf_tweet.loc[(df_tweet.composite_score<0.05)&(df_tweet.composite_score>-0.05),'sentiment'] = 'neutral'\ndf_tweet.sentiment.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet.head()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet['created_at'] = pd.to_datetime(df_tweet['created_at'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df_tweet.loc[df_tweet.sentiment=='positive','created_at'].value_counts()\npd.crosstab(index=df_tweet['created_at'], columns=df_tweet['sentiment']).plot.bar(stacked=True,title='tweet sentiment top 5 countries (US|CA|IN|ES|GB)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index=df_india['created_at'], columns=df_india['sentiment']).plot.bar()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_india = df_tweet[df_tweet.country_code=='IN']\npd.crosstab(index=df_india['created_at'], columns=df_india['sentiment']).plot.bar(stacked=True,title='tweet sentiment for INDIA')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.crosstab(index=df_tweet['country_code'], columns=df_tweet['sentiment']).plot.bar(stacked=True,title='tweet sentiment for top 5 countries (US|CA|IN|ES|GB)')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet.columns\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet[df_tweet.country_code=='IN'].groupby(['place_type'])['sentiment'].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def Text_Cleaning(data, remove_numbers = True, custom_stopwords = None , remove_stop_words=True, lemmatize_words = False, stem_words= False):\n\n    \"\"\"\n\n    Text Cleaning  \n    Input: List of Documents    Output: Cleaned List of Documents\n    Parameters: \n    custom_stopwords - Custom List of Stopwords(should be passed as a list)\n    remove_stop_words - Binary parameter (True/False) to remove stopwords or do not want to perform this. Default is True\n    lemmatize_words - Binary parameter (True/False) for Lemmatization (WordNet Lemmatizer used) or not. Default is False\n    stem_words - Binary parameter (True/False) for Stemming (Lancaster Stemmer used) or not. Default is False\n\n    \"\"\"\n    import os\n\n    package = \"nltk\"\n\n    try:\n        __import__package\n    except:\n        os.system(\"pip install \"+ package)\n        import nltk\n        nltk.download('stopwords')\n        nltk.download('wordnet')\n    \n    import nltk\n    import pandas as pd\n    import string\n    \n    \n\n    import re\n    data = [re.sub(\"^\\s+\",\"\",x) for x in data] # removes leading white spaces\n    data = [re.sub(\"\\s+\\Z\",\"\",x) for x in data] # removes trailing white spaces\n\n    data = [x.lower() for x in data]\n    data = [x.translate(str.maketrans(' ',' ', '!\"#$%&\\'()*+,./:;<=>?@[\\\\]^`{|}~_')) for x in data]\n    data = [x.replace('-', ' ') for x in data]\n    \n    if remove_numbers:\n        data = [x.translate(str.maketrans('','', string.digits)) for x in data]\n\n\n    ### Tokenizer\n    def tokenize_word_text(text):\n        \"\"\"\n        Sentence tokenize in NLTK with sent_tokenize \n        The sent_tokenize function uses an instance of NLTK known as PunktSentenceTokenizer\n        This instance of NLTK has already been trained to perform tokenization on different European languages on the basis \n        of letters or punctuation that mark the beginning and end of sentences\n        \"\"\"\n        from nltk.tokenize import WordPunctTokenizer \n        tokenizer = WordPunctTokenizer()\n        return tokenizer.tokenize(text)\n\n\n    ## Removing Stopwords\n    if remove_stop_words:\n        roman_numerals = ['iii', 'iiv', 'iiiv', 'vii', 'viii', 'xii', 'xiii', 'xxi','xxii', 'xxiii', 'xxv', 'xxvi', 'xxvii'] #exclude 2char numerals\n        temporal_words = ['coronavirus','india','covid_19','hour', 'day', 'week', 'month', 'quarter', 'year', 'annual', 'today', 'tomorrow', 'yesterday', \n                          'prior', 'prio', 'preceed', 'yet', \"before\", 'after', 'earlier', 'later', 'ago', 'this', 'next', 'last',\n                          'once-daili', 'once-month', 'once-through', 'once-week', 'once', 'also','covid 19','corona','co',                          \n                          'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday','coronaviru','http','http co',\n                          'jan', 'feb', 'apr', 'jun', 'jul', 'aug', 'sept', 'oct', 'nov', 'dec', 'may', 'onmay', 'postmay'\n                         ,'corona virus','pandemic']\n        from nltk.corpus import stopwords\n        stopword_list = stopwords.words('english')\n        if custom_stopwords:\n            stop_list = set(stopword_list + custom_stopwords + roman_numerals + temporal_words)\n        else:\n            stop_list = set(stopword_list + roman_numerals + temporal_words)\n\n        def removeStopwords_before_tokens(text, stopwords):\n            tokens = tokenize_word_text(text)\n            return ' '.join([w for w in tokens if w not in stopwords])\n\n        data = [removeStopwords_before_tokens(x, stop_list) for x in data]\n\n\n\n    ##### Word Lemmatization Using WordNet\n    def wordlemmatize(text):\n        from nltk.stem.wordnet import WordNetLemmatizer \n        lem = WordNetLemmatizer()\n        tokens = tokenize_word_text(text)\n        return ' '.join([lem.lemmatize(w) for w in tokens])\n    if lemmatize_words:\n        data = [wordlemmatize(x) for x in data]\n\n\n\n    ##### Word Stemming Using Lancaster Stemmer\n    def wordstem(text):\n        from nltk.stem.porter import PorterStemmer\n        ls = PorterStemmer()\n        tokens = tokenize_word_text(text)\n        return ' '.join([ls.stem(w) for w in tokens])\n    if stem_words:\n        data = [wordstem(x) for x in data]\n\n\n   \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_tweet['clean_tweet'] = Text_Cleaning(df_tweet['text'],lemmatize_words = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub_economy = df_tweet[df_tweet.clean_tweet.str.contains('economy')]\nsub_economy.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.max_colwidth', 100)\n# Start with one review:\ndf_temp = df_tweet\ndf_ADR = df_temp[df_temp['sentiment']=='positive']\ndf_NADR = df_temp[df_temp['sentiment']=='negative']\ntweet_All = \" \".join(review for review in df_temp.clean_tweet)\ntweet_ADR = \" \".join(review for review in df_ADR.clean_tweet)\ntweet_NADR = \" \".join(review for review in df_NADR.clean_tweet)\n\nfig, ax = plt.subplots(3, 1, figsize  = (30,30))\n# Create and generate a word cloud image:\nwordcloud_ALL = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_All)\nwordcloud_ADR = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_ADR)\nwordcloud_NADR = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(tweet_NADR)\n\n# Display the generated image:\nax[0].imshow(wordcloud_ALL, interpolation='bilinear')\nax[0].set_title('All Tweets', fontsize=30)\nax[0].axis('off')\nax[1].imshow(wordcloud_ADR, interpolation='bilinear')\nax[1].set_title('Tweets under positive Class',fontsize=30)\nax[1].axis('off')\nax[2].imshow(wordcloud_NADR, interpolation='bilinear')\nax[2].set_title('Tweets under negative Class',fontsize=30)\nax[2].axis('off')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud.to_file(\"img/first_review_usa.png\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}